{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8975073e-3f07-4e65-b957-3a686c4c3195",
   "metadata": {},
   "source": [
    "# 10.4.1 TD(0) Prediction\n",
    "\n",
    "## Explanation of TD(0) Prediction\n",
    "\n",
    "TD(0) Prediction is a method used in reinforcement learning to estimate the value function of a policy. It combines ideas from both Monte Carlo methods and Dynamic Programming. Unlike Monte Carlo, which waits until the end of an episode to update values, TD(0) updates the value estimates after every step. This makes TD(0) an online method, which is useful for learning from continuous tasks.\n",
    "\n",
    "The central idea of TD(0) is to update the value of a state based on the observed reward and the estimated value of the next state. This update rule is known as the TD(0) update rule:\n",
    "\n",
    "$$ V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)] $$\n",
    "\n",
    "Where:\n",
    "- $V(s)$ is the current estimate of the value of state $s$.\n",
    "- $r$ is the reward received after taking action $a$ in state $s$.\n",
    "- $s'$ is the next state after taking action $a$.\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\gamma$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254b1b9-ae8f-4f92-bcd0-f7ddd081c327",
   "metadata": {},
   "source": [
    "## Algorithm for Implementing TD(0) Prediction\n",
    "\n",
    "To implement TD(0) Prediction, the following steps are followed:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the value function $V(s)$ arbitrarily for all states $s$.\n",
    "   - Choose a small learning rate $\\alpha$ and a discount factor $\\gamma$.\n",
    "\n",
    "2. **Policy Execution**:\n",
    "   - Follow a policy $\\pi$ to interact with the environment.\n",
    "   - For each state $s$, take an action $a$, observe the reward $r$, and transition to the next state $s'$.\n",
    "\n",
    "3. **Value Update**:\n",
    "   - Apply the TD(0) update rule: \n",
    "     $$ V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)] $$\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Repeat the above steps for multiple episodes or until the value function converges.\n",
    "\n",
    "This method allows for continuous learning and improvement of the value function as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4291b8-5ce7-4b26-a3c4-4388460b252f",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Temporal Difference Learning in Reinforcement Learning](https://medium.com/nerd-for-tech/temporal-difference-learning-in-reinforcement-learning-cf13ed159fcb)\n",
    "- [Introduction to Temporal Difference (TD) Learning](https://medium.com/analytics-vidhya/nuts-and-bolts-of-reinforcement-learning-introduction-to-temporal-difference-td-learning-a0624eb3b985)\n",
    "- [Temporal-Difference Learning and the importance of exploration](https://towardsdatascience.com/temporal-difference-learning-and-the-importance-of-exploration-an-illustrated-guide-5f9c3371413a)\n",
    "- [Temporal Difference Learning â€” Part 1](https://readmedium.com/en/https:/medium.com/analytics-vidhya/reinforcement-learning-temporal-difference-learning-part-1-339fef103850)\n",
    "- [Simple Reinforcement Learning: Temporal Difference Learning](https://readmedium.com/en/https:/medium.com/@violante.andre/simple-reinforcement-learning-temporal-difference-learning-e883ea0d65b0)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c87c2-5a37-4b52-87c0-3363b038fda9",
   "metadata": {},
   "source": [
    "## Benefits and Use Cases of TD(0) Prediction\n",
    "\n",
    "### Benefits:\n",
    "1. **Efficiency**: TD(0) updates value estimates after each step, making it more efficient than Monte Carlo methods, which update values only after the end of an episode.\n",
    "2. **Applicability to Continuous Tasks**: TD(0) can be used in situations where episodes do not naturally end, making it suitable for ongoing tasks.\n",
    "3. **Low Variance**: Since TD(0) updates values incrementally, it has lower variance compared to Monte Carlo methods, which rely on the return of the entire episode.\n",
    "\n",
    "### Use Cases:\n",
    "- **Real-Time Systems**: TD(0) is particularly useful in systems where updates need to be made in real-time, such as in robotics or financial trading.\n",
    "- **Games**: In games where the goal is to improve the strategy continuously, TD(0) can be used to refine the value estimates of different states during the gameplay.\n",
    "- **Continuous Control**: TD(0) can be applied to problems in continuous control, where the environment does not have a clear episodic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514de1a-127c-4ea7-bc25-e6b19c8f6356",
   "metadata": {},
   "source": [
    "Here's a simple Python implementation of TD(0) Prediction for a basic environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fecb584-37f7-47c9-8072-787574045f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4df2ebe-eed2-4064-9780-c513f63b1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "n_states = 5      # Number of states\n",
    "gamma = 0.9       # Discount factor\n",
    "alpha = 0.1       # Learning rate\n",
    "n_episodes = 100  # Number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845f8030-5f45-4574-bfd6-3639dd9753ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value function\n",
    "V = np.zeros(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80b1e27-dffd-499a-bc85-4e636cd270a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy \n",
    "# Assuming a random policy for simplicity\n",
    "def policy(state):\n",
    "    return np.random.choice([0, 1])  # 0 = left, 1 = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d001e0a-6489-4f4e-9a41-85daab236801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "def reward(state):\n",
    "    if state == n_states - 1:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b0d4c9-7610-49be-af6b-07150b51f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(state, action):\n",
    "    if action == 1:  # Move right\n",
    "        return min(state + 1, n_states - 1)\n",
    "    else:            # Move left\n",
    "        return max(state - 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e616972-02c8-4de6-aabc-66328d038e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(0) Prediction\n",
    "for episode in range(n_episodes):\n",
    "    state = np.random.randint(0, n_states)  # Start from a random state\n",
    "    while state != n_states - 1:  # Continue until reaching the terminal state\n",
    "        action = policy(state)\n",
    "        next_s = next_state(state, action)\n",
    "        r = reward(next_s)\n",
    "        V[state] = V[state] + alpha * (r + gamma * V[next_s] - V[state])\n",
    "        state = next_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e58b99d-125e-47ad-a8d1-75be543fb318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Value Function: [0.22615379 0.27824649 0.43014637 0.6640568  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Display the learned value function\n",
    "print(\"Learned Value Function:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460bc05-c7ab-4b70-8777-3bedd11948b2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**TD(0) Prediction** provides a powerful yet straightforward way to estimate the value function in reinforcement learning. By updating the value function **after each step** based on the observed rewards and the estimated value of the next state, TD(0) combines the advantages of both **Monte Carlo methods (which require complete episodes)** and **Dynamic Programming (which requires a model of the environment)**. This method is particularly useful in environments where full episodes may be lengthy or difficult to obtain, making it a versatile tool for learning optimal policies in various reinforcement learning tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
