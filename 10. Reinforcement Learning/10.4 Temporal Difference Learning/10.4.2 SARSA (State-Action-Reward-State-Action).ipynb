{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6940f92c-f879-499e-8841-4e6b5ac8fdbe",
   "metadata": {},
   "source": [
    "# 10.4.2 SARSA (State-Action-Reward-State-Action)\n",
    "\n",
    "### Explanation of SARSA (State-Action-Reward-State-Action)\n",
    "\n",
    "SARSA is an on-policy reinforcement learning algorithm that updates the action-value function based on the current state, the action taken, the reward received, the next state, and the next action. The acronym SARSA stands for State-Action-Reward-State-Action, which captures the sequence of events in the algorithm.\n",
    "\n",
    "The update rule for SARSA is given by:\n",
    "\n",
    "$$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] $$\n",
    "\n",
    "Where:\n",
    "- $ s_t $ and $ a_t $ are the current state and action, respectively.\n",
    "- $ r_{t+1} $ is the reward received after taking action $ a_t $ in state $ s_t $.\n",
    "- $ s_{t+1} $ and $ a_{t+1} $ are the next state and the next action, respectively.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\gamma $ is the discount factor.\n",
    "\n",
    "SARSA is an on-policy algorithm, meaning it learns the value of the policy being followed, including the exploration strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc018f2-2a7d-4437-b60d-2727e7e06115",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "### Methods for Implementing SARSA\n",
    "\n",
    "1. **Initialize the Q-Table**: Start with an initial Q-table with all values set to zero or a small random value.\n",
    "2. **Choose an Action**: Use an epsilon-greedy policy to choose an action based on the current state and the Q-values.\n",
    "3. **Take the Action and Observe the Outcome**: Execute the chosen action, observe the next state and the reward received.\n",
    "4. **Choose the Next Action**: Again, use the epsilon-greedy policy to choose the next action based on the new state.\n",
    "5. **Update the Q-Value**: Update the Q-value for the current state-action pair using the SARSA update rule.\n",
    "6. **Repeat**: Continue this process until convergence or for a predefined number of episodes.\n",
    "\n",
    "This step-by-step process will eventually lead to the learning of an optimal or near-optimal policy, allowing the agent to make better decisions in the given environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa976c90-d983-4816-8e92-e88d59ebb723",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Reinforcement Learning: Temporal Difference Learning — Part 2](https://readmedium.com/en/https:/medium.com/analytics-vidhya/reinforcement-learning-temporal-difference-learning-part-2-c290af52f483)\n",
    "- [The Epsilon-Greedy Algorithm for Reinforcement Learning](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870)\n",
    "- [Temporal differencing (SARSA, Q learning, Expected SARSA)](https://medium.com/@j13mehul/reinforcement-learning-part-6-temporal-differencing-sarsa-q-learning-expected-sarsa-b7725c755410)\n",
    "- [Reinforcement Learning: SARSA](https://readmedium.com/en/https:/towardsdev.com/reinforcement-learning-sarsa-1a703f0cb25b)\n",
    "- [Reinforcement Learning with SARSA — A Good Alternative to Q-Learning Algorithm](https://readmedium.com/en/https:/towardsdatascience.com/reinforcement-learning-with-sarsa-a-good-alternative-to-q-learning-algorithm-bf35b209e1c)\n",
    "- [Navigating Complexity: The Role of SARSA in Reinforcement Learning for Game Strategy Optimization](https://readmedium.com/en/https:/pub.aimind.so/navigating-complexity-the-role-of-sarsa-in-reinforcement-learning-for-game-strategy-optimization-6dff9e630453)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de602651-1197-46c7-aa7f-59fee97f6112",
   "metadata": {},
   "source": [
    "### Benefits and Scenarios for Using SARSA\n",
    "\n",
    "- **On-Policy Learning**: SARSA directly learns the policy being used to make decisions. This makes it ideal for environments where the exploration strategy is an integral part of the decision-making process.\n",
    "- **Exploration-Sensitive**: Because SARSA considers the action actually taken in the next state, it takes into account the exploration policy. This can lead to safer behavior in some scenarios compared to Q-learning, which is an off-policy method.\n",
    "- **Stability**: SARSA can be more stable than Q-learning in environments with high variability or noise because it updates using the actions chosen by the current policy rather than the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950312e3-5426-4c9b-9244-d205dc69d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec2e0d8-573b-4c90-b688-2b7cfd8f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "n_states = 6   # Number of states\n",
    "n_actions = 2  # Number of actions (0 = left, 1 = right)\n",
    "alpha = 0.1    # Learning rate\n",
    "gamma = 0.9    # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "n_episodes = 1000  # Number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fcac0bc-f76b-4d6e-8e50-5873d0fca220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q = defaultdict(lambda: np.zeros(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef7eb79-4d4b-4339-8f10-984b484a64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    # Choose an action based on epsilon-greedy policy.\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(np.arange(n_actions))  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])                  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cb479d-756e-43b3-b2f3-d2f33623fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    # SARSA algorithm implementation.\n",
    "    for _ in range(n_episodes):\n",
    "        state = np.random.randint(0, n_states - 1)  # Initialize the starting state\n",
    "        action = epsilon_greedy_policy(state)       # Choose the first action\n",
    "        \n",
    "        while state != n_states - 1:  \n",
    "            next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "            reward = 1.0 if next_state == n_states - 1 else 0.0\n",
    "            \n",
    "            next_action = epsilon_greedy_policy(next_state)\n",
    "            \n",
    "            # Update Q-value using the SARSA update rule\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    \n",
    "    optimal_policy = {state: np.argmax(Q[state]) for state in range(n_states)}\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3bba40e-1b47-4913-9bbb-5c4aa9781c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 0}\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = sarsa()\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e76ecb-2acc-4336-bde5-70a2c92f193a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section, we explored the SARSA (State-Action-Reward-State-Action) algorithm, a key method in reinforcement learning for on-policy temporal difference control. We implemented SARSA in a simple grid world environment, demonstrating how the agent learns to take optimal actions through exploration and exploitation. The epsilon-greedy policy allowed the agent to balance the trade-off between exploration of new actions and exploitation of known rewards. The final result was an optimal policy derived from the Q-table, showcasing the effectiveness of SARSA in finding the best strategy in this scenario. This foundational understanding of SARSA prepares us to tackle more complex reinforcement learning problems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
