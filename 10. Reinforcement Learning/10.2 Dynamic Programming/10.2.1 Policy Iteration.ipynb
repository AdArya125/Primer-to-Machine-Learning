{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49bb2b55-89c8-41ab-bb6f-d1bfa685e726",
   "metadata": {},
   "source": [
    "## 10.2.1 Policy Iteration\n",
    "\n",
    "### Explanation of Policy Iteration\n",
    "\n",
    "Policy Iteration is a fundamental algorithm in Reinforcement Learning used to solve **Markov Decision Processes (MDPs)**. It involves two main steps: Policy Evaluation and Policy Improvement. The goal is to find an optimal policy that maximizes the expected cumulative reward for an agent interacting with the environment.\n",
    "\n",
    "1. **Policy Evaluation:** Given a policy (a mapping from states to actions), this step calculates the value function, which represents the expected return (or reward) from each state under the current policy.\n",
    "\n",
    "2. **Policy Improvement:** After evaluating the policy, this step updates the policy by choosing actions that maximize the value function. This means the agent will prefer actions that lead to higher rewards, thus improving the policy.\n",
    "\n",
    "The algorithm alternates between these two steps until the policy converges to an optimal one, where no further improvements can be made.\n",
    "\n",
    "### Benefits and Use Cases of Policy Iteration\n",
    "\n",
    "- **Guaranteed Convergence:** Policy Iteration is guaranteed to converge to an optimal policy for finite MDPs, making it a reliable algorithm for finding the best strategy in environments with known dynamics.\n",
    "  \n",
    "- **Efficiency:** Compared to value iteration, Policy Iteration often requires fewer iterations to converge, especially in environments where the policy does not change frequently after evaluation.\n",
    "  \n",
    "- **Applications:** Policy Iteration is used in various fields, including robotics, game playing, and resource management, where the environment can be modeled as an MDP, and the goal is to find an optimal decision-making strategy.\n",
    "\n",
    "### Methods for Implementing Policy Iteration\n",
    "\n",
    "To implement Policy Iteration, follow these steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with an arbitrary policy and initialize the value function for all states to zero or random values.\n",
    "\n",
    "2. **Policy Evaluation:**\n",
    "   - Iterate over all states and update the value function by computing the expected return under the current policy.\n",
    "\n",
    "3. **Policy Improvement:**\n",
    "   - For each state, update the policy by selecting the action that maximizes the expected return based on the current value function.\n",
    "\n",
    "4. **Convergence:**\n",
    "   - Repeat the Policy Evaluation and Policy Improvement steps until the policy converges and no further improvements can be made.\n",
    "\n",
    "The following Python code snippet provides an example of how to implement Policy Iteration for a simple MDP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c761c0-3f2d-4d39-b779-f9466b9eb5dc",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Finite Markov Decision Processes](https://medium.com/towards-data-science/introduction-to-reinforcement-learning-rl-part-3-finite-markov-decision-processes-51e1f8d3ddb7)\n",
    "- [Dynamic Programming](https://medium.com/towards-data-science/introduction-to-reinforcement-learning-rl-part-4-dynamic-programming-6af57e575b3d)\n",
    "- [Policy Iteration in RL: A step by step Illustration](https://towardsdatascience.com/policy-iteration-in-rl-an-illustration-6d58bdcb87a7)\n",
    "- [Policy Iteration — Easy Example](https://medium.com/@pesupavish/policy-iteration-easy-example-d3fd1eb98c6c)\n",
    "- [Reinforcement Learning Chapter 4: Dynamic Programming \\(Part 1 — Policy Iteration\\)](https://medium.com/@numsmt2/reinforcement-learning-chapter-4-dynamic-programming-part-1-policy-iteration-2a1f66a5ca42)\n",
    "- [Markov decision process: policy iteration with code implementation](https://medium.com/@ngao7/markov-decision-process-policy-iteration-42d35ee87c82)\n",
    "- [Policy iteration](https://gibberblot.github.io/rl-notes/single-agent/policy-iteration.html)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2427b7a-fc78-4e6a-9c24-81e63aa303c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [1 0 0 0]\n",
      "Optimal Value Function: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the MDP environment\n",
    "states = [0, 1, 2, 3]  # States\n",
    "actions = [0, 1]       # Actions: 0 = left, 1 = right\n",
    "rewards = np.array([[-1, 0], [0, 0], [0, 0], [0, 1]])  # Reward matrix\n",
    "transition_probs = np.array([\n",
    "    [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]],  # State 0\n",
    "    [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],  # State 1\n",
    "    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]],  # State 2\n",
    "    [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]   # State 3 (Terminal)\n",
    "])\n",
    "\n",
    "# Initialize policy and value function\n",
    "policy = np.zeros(len(states), dtype=int)\n",
    "value_function = np.zeros(len(states))\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold\n",
    "\n",
    "def policy_evaluation(policy, value_function, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = value_function[s]\n",
    "            action = policy[s]\n",
    "            value_function[s] = sum(transition_probs[s, action, s_prime] * \n",
    "                                    (rewards[s, action] + gamma * value_function[s_prime])\n",
    "                                    for s_prime in states)\n",
    "            delta = max(delta, abs(v - value_function[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "def policy_improvement(policy, value_function, gamma):\n",
    "    policy_stable = True\n",
    "    for s in states:\n",
    "        old_action = policy[s]\n",
    "        action_values = np.zeros(len(actions))\n",
    "        for a in actions:\n",
    "            action_values[a] = sum(transition_probs[s, a, s_prime] * \n",
    "                                   (rewards[s, a] + gamma * value_function[s_prime])\n",
    "                                   for s_prime in states)\n",
    "        policy[s] = np.argmax(action_values)\n",
    "        if old_action != policy[s]:\n",
    "            policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "# Policy Iteration Algorithm\n",
    "while True:\n",
    "    value_function = policy_evaluation(policy, value_function, gamma, theta)\n",
    "    policy, policy_stable = policy_improvement(policy, value_function, gamma)\n",
    "    if policy_stable:\n",
    "        break\n",
    "\n",
    "print(\"Optimal Policy:\", policy)\n",
    "print(\"Optimal Value Function:\", value_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd86d9-becd-4e00-b2c5-6c8f554e915c",
   "metadata": {},
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "In this section, we explored the concept of Policy Iteration in Reinforcement Learning, a dynamic programming method used to find the optimal policy in Markov Decision Processes (MDPs). By alternating between policy evaluation and policy improvement, Policy Iteration efficiently converges to an optimal policy that maximizes the expected cumulative reward. We also implemented a basic example to illustrate how this method can be applied in practice. This foundational understanding of Policy Iteration provides a strong basis for more advanced reinforcement learning techniques.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
