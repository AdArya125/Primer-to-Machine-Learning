{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a1c1b5-00de-444a-8b9a-d13521aa7d05",
   "metadata": {},
   "source": [
    "# 10.2.2 Value Iteration\n",
    "\n",
    "## Explanation of Value Iteration\n",
    "\n",
    "Value Iteration is an algorithm used in reinforcement learning to find the optimal policy for a Markov Decision Process (MDP). Unlike Policy Iteration, which involves separate steps for policy evaluation and policy improvement, Value Iteration combines these two steps into a single process. The algorithm iteratively updates the value function for each state by considering the expected utility of taking an action and moving to the next state, ultimately converging to the optimal value function. Once the optimal value function is found, the optimal policy can be derived by choosing the action that maximizes the expected value.\n",
    "\n",
    "## Benefits and Scenarios for Using Value Iteration\n",
    "\n",
    "- **Efficiency:** Value Iteration is often more efficient than Policy Iteration because it combines the evaluation and improvement steps, leading to faster convergence.\n",
    "- **Convergence:** The algorithm guarantees convergence to the optimal policy, making it a reliable method for solving MDPs.\n",
    "- **Applicability:** Value Iteration is well-suited for scenarios where the state space is manageable, and the MDP is well-defined, such as grid-based environments and other finite-state problems.\n",
    "- **Flexibility:** The algorithm can be adapted to various settings, including partially observable MDPs (POMDPs) and stochastic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1d55a-8d9b-4827-8353-889f3a22d083",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Reinforcement Learning Chapter 4: Dynamic Programming (Part 3 â€” Value Iteration)](https://medium.com/@numsmt2/reinforcement-learning-chapter-4-dynamic-programming-part-3-value-iteration-6f01f6347813)\n",
    "- [Markov decision process: value iteration with code implementation](https://medium.com/@ngao7/markov-decision-process-value-iteration-2d161d50a6ff)\n",
    "- [Reinforcement Learning: an Easy Introduction to Value Iteration](https://towardsdatascience.com/reinforcement-learning-an-easy-introduction-to-value-iteration-e4cfe0731fd5)\n",
    "- [Value Iteration](https://gibberblot.github.io/rl-notes/single-agent/value-iteration.html)\n",
    "- [Value Iteration vs. Policy Iteration in Reinforcement Learning](https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration)\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7866fed-adcd-4882-a419-1cc6db6fcb1a",
   "metadata": {},
   "source": [
    "\n",
    "## Methods for Implementing Value Iteration\n",
    "\n",
    "Below is a basic implementation of Value Iteration in Python. This example considers a simple grid environment where the agent seeks to find the optimal path to a goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178a2906-41f3-411d-bef3-f90d4d74edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "State (0, 0): 88.99916647515822\n",
      "State (0, 1): 99.99916647515822\n",
      "State (0, 2): 99.99916647515822\n",
      "State (1, 0): 80.0992498276424\n",
      "State (1, 1): 88.9992498276424\n",
      "State (1, 2): 99.9992498276424\n",
      "State (2, 0): 71.08932484487816\n",
      "State (2, 1): 79.09932484487817\n",
      "State (2, 2): 88.99932484487816\n",
      "\n",
      "Optimal Policy:\n",
      "State (0, 0): right\n",
      "State (0, 1): right\n",
      "State (0, 2): up\n",
      "State (1, 0): up\n",
      "State (1, 1): right\n",
      "State (1, 2): up\n",
      "State (2, 0): up\n",
      "State (2, 1): right\n",
      "State (2, 2): up\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "states = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "rewards = {\n",
    "    (0, 0): 0, (0, 1): -1, (0, 2): 10,\n",
    "    (1, 0): -1, (1, 1): -1, (1, 2): -1,\n",
    "    (2, 0): -1, (2, 1): -1, (2, 2): -1\n",
    "}\n",
    "transitions = {\n",
    "    'up': (-1, 0), 'down': (1, 0),\n",
    "    'left': (0, -1), 'right': (0, 1)\n",
    "}\n",
    "gamma = 0.9  # Discount factor\n",
    "theta = 0.0001  # Threshold for stopping criterion\n",
    "\n",
    "# Initialize value function\n",
    "V = {state: 0 for state in states}\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    next_state = (state[0] + transitions[action][0], state[1] + transitions[action][1])\n",
    "    return next_state if next_state in states else state\n",
    "\n",
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v = V[state]\n",
    "            action_values = []\n",
    "            for action in actions:\n",
    "                next_state = get_next_state(state, action)\n",
    "                action_value = rewards[next_state] + gamma * V[next_state]\n",
    "                action_values.append(action_value)\n",
    "            V[state] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "def get_policy():\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            next_state = get_next_state(state, action)\n",
    "            action_values[action] = rewards[next_state] + gamma * V[next_state]\n",
    "        policy[state] = max(action_values, key=action_values.get)\n",
    "    return policy\n",
    "\n",
    "# Perform Value Iteration\n",
    "value_iteration()\n",
    "optimal_policy = get_policy()\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "for state in states:\n",
    "    print(f\"State {state}: {V[state]}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state in states:\n",
    "    print(f\"State {state}: {optimal_policy[state]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a55e8-7822-4650-8717-e501d6a5467f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Value Iteration is a fundamental method in reinforcement learning that iteratively improves the value function for each state until it converges to the optimal value function. By evaluating the expected rewards for each possible action and selecting the maximum, Value Iteration provides a way to determine the optimal policy for an agent in a given environment. This process is particularly useful in environments with a finite number of states and actions, where the goal is to maximize cumulative rewards over time. Through the implementation of Value Iteration, we can systematically approach the problem of decision-making in uncertain environments, ensuring that the agent makes the best possible choices to achieve its objectives.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
