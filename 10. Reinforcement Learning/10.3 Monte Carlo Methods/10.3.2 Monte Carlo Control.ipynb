{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7169aabb-c036-4a35-b4a5-b1d87a840d72",
   "metadata": {},
   "source": [
    "# 10.3.2 Monte Carlo Control\r\n",
    "\r",
    "### Explanation of Monte Carlo Control\r\n",
    "\r\n",
    "Monte Carlo Control is an extension of Monte Carlo methods in reinforcement learning that focuses on optimizing the policy to maximize the expected reward. Unlike Monte Carlo Prediction, which estimates the value of a given policy, Monte Carlo Control iteratively improves the policy itself by exploring the environment and adjusting actions to achieve better outcomes.\r\n",
    "\r\n",
    "The core idea behind Monte Carlo Control is to use the results from simulated episodes to not only evaluate the value function but also to derive an improved policy. This is achieved through policy iteration, where the policy is updated based on the value estimates, and then more episodes are run under the new policy to further refine i.\r\n",
    "\r\n",
    "### Scenarios Where Monte Carlo Control is Beneficial\r\n",
    "\r\n",
    "Monte Carlo Control is particularly beneficial in scenarios where:\r\n",
    "\r\n",
    "- **Model-Free Environments**: The environment's dynamics are unknown, and it is not feasible to derive an analytical solution. Monte Carlo methods work well because they rely solely on experience.\r\n",
    "- **Episodic Tasks**: Tasks that naturally break into episodes, such as games, where each episode has a clear beginning and end.\r\n",
    "- **High Variance in Rewards**: When the reward structure is complex and varies significantly between episodes, Monte Carlo methods can accurately estimate the average value over time.\r\n",
    "- **Limited State Space**: Although Monte Carlo Control can be used with large state spaces, it is most effective when the state space is relatively small, making it feasible to explore horoughly.\r\n",
    "\r\n",
    "### Methods for Implementing Monte Carlo Control\r\n",
    "\r\n",
    "Monte Carlo Control can be implemented using different strategies. The most common methods are:\r\n",
    "\r\n",
    "- **Exploring Starts**: Ensuring that all state-action pairs are visited by starting episodes with a random action.\r\n",
    "- **$\\epsilon$-Greedy Policy**: Balancing exploration and exploitation by selecting the best-known action most of the time but occasionally choosing a random action.\r\n",
    "- **Policy Iteration**: Alternating between policy evaluation (estimating the value function under the current policy) and policy improvement (updating the policy based on the value estimates).\r\n",
    "\r\n",
    "These methods can be used to incrementally improve the policy, eventually converging to an optimal policy that maximizes the expected reward over time.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e560ee-1c24-4e14-bb93-333171283dc8",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Monte Carlo Methods (Part 2: Monte Carlo Control)](https://medium.com/@numsmt2/reinforcement-learning-chapter-5-monte-carlo-methods-part-2-monte-carlo-control-b1ea0d4ec2b4)\n",
    "- [Reinforcement Learning, Part 4: Monte Carlo Control](https://towardsdatascience.com/reinforcement-learning-part-4-monte-carlo-control-ae0a7f29920b)\n",
    "- [Monte Carlo Control](https://medium.com/@aminakeldibek/monte-carlo-control-6e3b70f173a8)\n",
    "- [Monte Carlo Methods](https://medium.com/towards-data-science/introduction-to-reinforcement-learning-rl-part-5-monte-carlo-methods-25067003bb0f)\n",
    "- [Solving Racetrack in Reinforcement Learning using Monte Carlo Control](https://towardsdatascience.com/solving-racetrack-in-reinforcement-learning-using-monte-carlo-control-bdee2aa4f04e)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5763c6d4-8214-42d6-9c19-9fbff623ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbcf1de-99f6-48b7-846e-ea57bfb71e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "n_states = 6   # Number of states\n",
    "n_actions = 2  # Number of actions (0 = left, 1 = right)\n",
    "gamma = 0.9    # Discount factor\n",
    "epsilon = 0.1  # Exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a985ae2d-10ba-4e55-a141-711af53d663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rewards\n",
    "rewards = np.zeros(n_states)\n",
    "rewards[-1] = 1.0         # Reward at the terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9faf6d-6b6e-4882-b6a0-283f740a4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-value table\n",
    "Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "\n",
    "max_steps = 100\n",
    "\n",
    "def generate_episode(policy):\n",
    "    state = np.random.randint(0, n_states - 1)\n",
    "    episode = []\n",
    "    for _ in range(max_steps):\n",
    "        if state == n_states - 1:\n",
    "            break\n",
    "        action = np.random.choice(np.arange(n_actions), p=policy[state])\n",
    "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "        reward = rewards[next_state]\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498d4505-bbe8-4c27-854c-08f44821e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create an epsilon-greedy policy\n",
    "def create_policy(Q, epsilon, n_actions):\n",
    "    policy = np.ones((n_states, n_actions)) * epsilon / n_actions\n",
    "    for state in range(n_states):\n",
    "        best_action = np.argmax(Q[state])\n",
    "        policy[state][best_action] += (1.0 - epsilon)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c04efb8-ea64-436a-8777-55055ac031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Control with epsilon-greedy policy\n",
    "n_episodes = 100  \n",
    "returns = defaultdict(list)\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    policy = create_policy(Q, epsilon, n_actions)\n",
    "    episode = generate_episode(policy)\n",
    "    G = 0\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = gamma * G + reward\n",
    "        if not any([(s == state and a == action) for (s, a, r) in episode[:-1]]):\n",
    "            returns[(state, action)].append(G)\n",
    "            Q[state][action] = np.mean(returns[(state, action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f61762-d400-4036-ba4c-af5bbb2ce2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the optimal policy by iterating over each state\n",
    "optimal_policy = np.zeros(n_states, dtype=int)\n",
    "for state in range(n_states):\n",
    "    optimal_policy[state] = np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2425b73-074d-4799-a1db-82a6b579f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Display the optimal policy\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a371b-e378-43ca-b4cd-e5c1f5747e93",
   "metadata": {},
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "In this implementation, we successfully demonstrated the Monte Carlo Control method with an epsilon-greedy policy. By iterating over each state and selecting the action with the highest Q-value, we derived the optimal policy for a simple grid world environment. This process highlights the essential aspects of Monte Carlo Control, including policy improvement through exploration and exploitation. The example serves as a foundational understanding of how Monte Carlo methods can be applied to reinforcement learning tasks, even in more complex environments.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
