{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b51d9d-a6b7-4d3f-a9e1-0b6d2a7b7cf6",
   "metadata": {},
   "source": [
    "# 10.3.1 Monte Carlo Prediction\r\n",
    "\r\n",
    "## Explanation of Monte Carlo Prediction\r\n",
    "\r\n",
    "Monte Carlo Prediction is a method used in reinforcement learning to estimate the value function for a particular policy. The value function represents the expected return (cumulative future rewards) from a given state or state-action pair under a specific policy. Unlike other methods, Monte Carlo Prediction relies on actual episodes of experience to calculate these estimates, rather than using a model of the environment.\r\n",
    "\r\n",
    "In Monte Carlo Prediction, the agent runs multiple episodes (complete sequences of states, actions, and rewards) by following a policy. After each episode, the agent calculates the total return from each state encountered during the episode and averages these returns over all episodes to estimate the value of that state under the policy. This method works well in episodic tasks where each episode has a clear start and end.\r\n",
    "\r\n",
    "## Benefits and Use Cases of Monte Carlo Prediction\r\n",
    "\r\n",
    "- **Model-Free:** Monte Carlo Prediction does not require a model of the environment, making it suitable for environments where the model is unknown or too complex to compute.\r\n",
    "  \r\n",
    "- **Handling Non-Markov Environments:** It can handle non-Markov environments (where the future depends on more than just the current state) by averaging over multiple episodes.\r\n",
    "\r\n",
    "- **Simple Implementation:** The algorithm is relatively simple to implement and understand, making it accessible for beginners in reinforcement learning.\r\n",
    "\r\n",
    "- **Accurate in the Long Run:** Given enough episodes, Monte Carlo methods can produce highly accurate value estimates for states under a specific policy.\r\n",
    "\r\n",
    "**Use Cases:**\r\n",
    "- **Gaming AI:** Estimating the value of positions in board games (like chess) after observing many game outcomes.\r\n",
    "- **Financial Modeling:** Predicting the future value of assets based on historical performance in different market conditions.\r\n",
    "- **Customer Lifetime Value:** Estimating the lifetime value of a customer based on observed behavior in marketing scenarios.\r\n",
    "\r\n",
    "## Methods for Implementing Monte Carlo Prediction\r\n",
    "\r\n",
    "Monte Carlo Prediction can be implemented using two primary approaches:\r\n",
    "\r\n",
    "1. **First-Visit Monte Carlo Method:** This method averages the returns of the first time a state is visited in each episode. For each state, only the first visit in each episode is considered, and the average of these first-visit returns is used as the estimate.\r\n",
    "\r\n",
    "2. **Every-Visit Monte Carlo Method:** This method averages the returns for every time a state is visited in an episode. All occurrences of a state in an episode are considered, and the average of all these returns is used as the estimate.\r\n",
    "\r\n",
    "### Implementation Steps:\r\n",
    "\r\n",
    "1. **Initialize the Value Function:** Start with an initial guess for the value function (e.g., set all values to zero).\r\n",
    "\r\n",
    "2. **Generate Episodes:** Run multiple episodes following the given policy, collecting the states, actions, and rewards.\r\n",
    "\r\n",
    "3. **Compute Returns:** For each state encountered during the episode, calculate the total return from that state until the end of the episode.\r\n",
    "\r\n",
    "4. **Update the Value Function:** Update the value estimate for each state by averaging the computed returns.\r\n",
    "\r\n",
    "5. **Iterate:** Repeat the process for many episodes to refine the value function estimates.\r\n",
    "\r\n",
    "This iterative process continues until the value function converges to a stable estimate, providing an accurate representation of the expected returns under the given policy.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86856e9-bb6b-4053-ab98-b05d96be9e7a",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [Monte Carlo in Reinforcement Learning](https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/)\n",
    "- [An Introduction and Step-by-Step Guide to Monte Carlo Simulations](https://medium.com/@benjihuser/an-introduction-and-step-by-step-guide-to-monte-carlo-simulations-4706f675a02f)\n",
    "- [Monte Carlo Methods](https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-5-monte-carlo-methods-25067003bb0f)\n",
    "- [Monte Carlo Methods for Reinforcement Learning](https://medium.com/nerd-for-tech/monte-carlo-methods-for-reinforcement-learning-d30d874dd817)\n",
    "- [Monte Carlo Methods (Part 1 â€” Monte Carlo Prediction)](https://medium.com/@numsmt2/reinforcement-learning-chapter-5-monte-carlo-methods-part-1-monte-carlo-prediction-fcc60c9ab726)\n",
    "- [Monte Carlo Methods](https://medium.com/neurosapiens/3-monte-carlo-methods-408c45699733)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f8a5a-c5a6-4e80-b130-2416410273f9",
   "metadata": {},
   "source": [
    "# Python Code for Monte Carlo Prediction\n",
    "\n",
    "Here is a simple implementation of Monte Carlo Prediction using the Every-Visit Monte Carlo method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a159f88-0e81-49ad-aca7-8cd77bd1fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede58fc5-1fc5-4842-b53e-ff215eef3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = [0, 1]  # 0: left, 1: right\n",
    "rewards = {0: 0, 1: 0, 2: 0, 3: 0, 4: 1}  # Reward is 1 when reaching the last state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fef6ee4-6f25-4838-a914-aecbe5a79834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an episode following a random policy\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state = np.random.choice(states)\n",
    "    while state != 4:  # Episode ends when reaching state 4\n",
    "        action = np.random.choice(actions)\n",
    "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "        reward = rewards[next_state]\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5ee08f-dd47-47f6-8e1f-cc7407ab923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Prediction using Every-Visit method\n",
    "def monte_carlo_prediction(num_episodes, gamma=1.0):\n",
    "    V = defaultdict(float)  # Initialize the value function\n",
    "    returns = defaultdict(list)  # Store returns for each state\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode()\n",
    "        G = 0  # Initialize the return\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if state not in [x[0] for x in episode[:t]]:\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315d1278-a123-49b8-8fda-56dea0d0a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function:\n",
      "State 3: 1.00\n",
      "State 2: 1.00\n",
      "State 1: 1.00\n",
      "State 0: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Run the Monte Carlo Prediction algorithm\n",
    "num_episodes = 1000\n",
    "value_function = monte_carlo_prediction(num_episodes)\n",
    "\n",
    "# Print the estimated value function\n",
    "print(\"Estimated Value Function:\")\n",
    "for state, value in value_function.items():\n",
    "    print(f\"State {state}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32fde11-7158-40bc-a38f-c1e06dca9983",
   "metadata": {},
   "source": [
    "# Conclusion\r\n",
    "\r\n",
    "In this section, we explored Monte Carlo Prediction, a key method in reinforcement learning used to estimate the value of states based on actual experiences or episodes. We highlighted its advantages, particularly its ability to work in environments where the model is not fully known, making it ideal for complex and episodic tasks. \r\n",
    "\r\n",
    "The provided Python code demonstrated how to implement the Every-Visit Monte Carlo method, where we repeatedly simulate episodes and use the observed rewards to estimate the value function. This technique offers a simple yet effective way to improve decision-making in reinforcement learning by leveraging empirical data.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
