{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917ff2e7-4b22-416c-94ec-872a1fe7c9e2",
   "metadata": {},
   "source": [
    "# 4.2.3.2 Gradient Boosting\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient Boosting is a powerful ensemble learning technique that builds a strong predictive model by sequentially adding weak learners (typically decision trees) to correct errors made by previous models. Key points include:\n",
    "\n",
    "- **Sequential Learning**: Models are added sequentially, and each new model corrects errors from its predecessor.\n",
    "- **Gradient Descent**: Gradient Boosting optimizes a loss function by descending gradients, minimizing prediction errors.\n",
    "- **Combining Weak Learners**: Boosting focuses on combining many weak learners to create a strong learner, improving model performance.\n",
    "- **Regularization**: Techniques like shrinkage (learning rate) and tree pruning prevent overfitting.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **High Accuracy**: Gradient Boosting often provides higher accuracy than individual models by leveraging the strengths of multiple weak learners.\n",
    "- **Handles Complex Relationships**: Effective in capturing complex relationships in data without overfitting.\n",
    "- **Feature Importance**: Provides insights into feature importance, aiding in model interpretation.\n",
    "- **Versatility**: Suitable for various types of data and tasks including classification, regression, and ranking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55259331-cfb2-45e8-8766-5612f9d54f90",
   "metadata": {},
   "source": [
    "## Overview of Different Gradient Boosting Libraries\n",
    "\n",
    "### XGBoost (Extreme Gradient Boosting)\n",
    "- **Developer**: Developed by Tianqi Chen.\n",
    "- **Key Features**:\n",
    "  - Parallelized tree boosting for scalability.\n",
    "  - Regularization techniques to prevent overfitting.\n",
    "  - Support for custom optimization objectives.\n",
    "  - High performance and speed optimization.\n",
    "- **Tasks**: Supports both classification and regression tasks.\n",
    "- **Use Case**: Widely used in competitions and industry for its performance and scalability.\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine)\n",
    "- **Developer**: Developed by Microsoft.\n",
    "- **Key Features**:\n",
    "  - Uses histogram-based algorithms for split finding, improving training speed.\n",
    "  - Efficient handling of large datasets and high-dimensional data.\n",
    "  - Native support for categorical features.\n",
    "  - High computational efficiency.\n",
    "- **Tasks**: Suitable for tasks requiring handling of large datasets and categorical features.\n",
    "- **Use Case**: Effective in scenarios where speed and efficiency are critical, such as large-scale data processing.\n",
    "\n",
    "### CatBoost (Categorical Boosting)\n",
    "- **Developer**: Developed by Yandex.\n",
    "- **Key Features**:\n",
    "  - Robust handling of categorical features with built-in support.\n",
    "  - Automatic handling of missing data.\n",
    "  - Feature importance analysis.\n",
    "  - Effective for heterogeneous data and high-cardinality categorical features.\n",
    "- **Tasks**: Designed specifically for datasets with categorical variables.\n",
    "- **Use Case**: Ideal for tasks involving complex categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e016ae-53a7-4417-b0fe-3da0220a2abe",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "### Readings:\n",
    "- [How Gradient Boosting Works](https://readmedium.com/en/https:/medium.com/@Currie32/how-gradient-boosting-works-76e3d7d6ac76)\n",
    "- [Ensemble techniques (Boosting â€” Gradient Boost)](https://towardsdev.com/machine-learning-algorithms-11-ensemble-techniques-boosting-gradient-boosting-697372be550a)\n",
    "- [XGBoost](https://medium.com/@myskill.id/xgboost-fa0a8547e197)\n",
    "- [The Math Behind XGBoost](https://readmedium.com/en/https:/medium.com/@cristianleo120/the-math-behind-xgboost-3068c78aad9d)\n",
    "- [LightGBM: A Comprehensive Guide](https://readmedium.com/en/https:/medium.com/@pelinokutan/lightgbm-a-comprehensive-guide-cb773cfc23b3)\n",
    "- [CatBoost Regression: Break It Down For Me](https://readmedium.com/en/https:/towardsdatascience.com/catboost-regression-break-it-down-for-me-16ed8c6c1eca)\n",
    "- [CatBoost Unleashed: Mastering Categorical Data for Robust Predictive Modeling](https://readmedium.com/en/https:/pub.aimind.so/catboost-unleashed-mastering-categorical-data-for-robust-predictive-modeling-ee081bf26f91)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5467b76e-a4c7-4e0e-8620-5d59a02f6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementing XGBoost in Python\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea663a86-726c-4db1-8711-da0a01f03aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementing LightGBM in Python\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the LightGBM classifier\n",
    "clf = lgb.LGBMClassifier(verbose=-1, num_leaves= 20)\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e43446-837d-4565-a719-165a6f164a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementing CatBoost in Python\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoost classifier\n",
    "clf = CatBoostClassifier(verbose=False)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69cd81-ac6b-4992-88f9-e3d840eaff4c",
   "metadata": {},
   "source": [
    "___\n",
    "## Conclusion\n",
    "\n",
    "Gradient Boosting algorithms, including XGBoost, LightGBM, and CatBoost, are powerful tools for building predictive models that excel in accuracy and efficiency. Key points to summarize:\n",
    "\n",
    "- **Performance**: Gradient Boosting algorithms leverage ensemble learning to achieve high accuracy by combining multiple weak learners.\n",
    "- **Scalability**: Libraries like XGBoost, LightGBM, and CatBoost are optimized for efficiency and can handle large datasets.\n",
    "- **Feature Importance**: They provide insights into feature importance, aiding in model interpretation and understanding.\n",
    "- **Applications**: Widely used in various domains including finance, healthcare, and marketing for tasks such as classification, regression, and ranking.\n",
    "\n",
    "In conclusion, Gradient Boosting algorithms are versatile and effective solutions for complex predictive modeling tasks, offering robust performance and interpretability across different applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
