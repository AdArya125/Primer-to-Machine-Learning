{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbf4153-939d-466b-ae89-b799c3cbf87b",
   "metadata": {},
   "source": [
    "# 4.2.4.1 Neural Networks\n",
    "\n",
    "## Explanation of Neural Networks\n",
    "\n",
    "Neural Networks are computational models inspired by the human brain's neural structure. They consist of interconnected layers of nodes (neurons), each performing simple computations that, when combined, can represent complex functions.\n",
    "\n",
    "## Types of Neural Networks\n",
    "\n",
    "1. **Multi-Layer Perceptron (MLP)**:\n",
    "   - **Structure**: Composed of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next layer.\n",
    "   - **Activation Functions**: Common activation functions include ReLU, sigmoid, and tanh.\n",
    "   - **Applications**: Used for various classification tasks such as image recognition, speech recognition, and text classification.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNN)**:\n",
    "   - **Structure**: Includes convolutional layers, pooling layers, and fully connected layers. Convolutional layers apply filters to capture spatial hierarchies in data.\n",
    "   - **Activation Functions**: Typically use ReLU activation.\n",
    "   - **Applications**: Widely used in image and video recognition, object detection, and image segmentation.\n",
    "\n",
    "3. **Recurrent Neural Networks (RNN)**:\n",
    "   - **Structure**: Contains loops allowing information to persist, making them suitable for sequential data.\n",
    "   - **Variants**: Includes Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks which address the vanishing gradient problem in standard RNNs.\n",
    "   - **Applications**: Used for tasks involving sequential data such as time series prediction, language modeling, and machine translation.\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c9164-826c-4ec9-a29a-d4f2e39aab40",
   "metadata": {},
   "source": [
    "### ***[Neural networks - 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)***\n",
    "### ***[Stanford CS 230 ― Deep Learning](https://stanford.edu/~shervine/teaching/cs-230/)***\n",
    "\n",
    "### Readings:\n",
    "- [The Math Behind Neural Networks](https://readmedium.com/en/https:/towardsdatascience.com/the-math-behind-neural-networks-a34a51b93873)\n",
    "- [The Math Behind Convolutional Neural Networks](https://readmedium.com/en/https:/towardsdatascience.com/the-math-behind-convolutional-neural-networks-6aed775df076)\n",
    "- [The Math Behind Recurrent Neural Networks](https://readmedium.com/en/https:/towardsdatascience.com/the-math-behind-recurrent-neural-networks-2de4e0098ab8)\n",
    "- [Explain Backpropagation from Mathematical Theory to Coding Practice](https://readmedium.com/en/https:/towardsdatascience.com/courage-to-learn-ml-explain-backpropagation-from-mathematical-theory-to-coding-practice-21e670415378)\n",
    "- [Backpropagation Through Time — How RNNs Learn](https://readmedium.com/en/https:/towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fb2c6-9445-42d4-b1fd-801fde1b69fb",
   "metadata": {},
   "source": [
    "## 1. **Multi-Layer Perceptron (MLP)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929c5cdb-16bf-4f06-83f4-43ae69585208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a90546-60de-484a-931f-c349b84e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00e6cd0-0f53-4a38-a004-1b8b6a96021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b20555-9eee-46be-87e4-f70a0f242a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca3c84d-e7ea-450a-a313-333c4ed0a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       1.00      1.00      1.00        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       0.97      0.97      0.97        34\n",
      "           4       0.98      1.00      0.99        46\n",
      "           5       0.98      0.96      0.97        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.97      1.00      0.98        30\n",
      "           9       0.95      0.95      0.95        40\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a4beb-7db0-4ee1-a325-53f396a5e726",
   "metadata": {},
   "source": [
    "## 2. **Convolutional Neural Networks (CNN)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effda662-15cb-4ced-9ec9-635992e6f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - accuracy: 0.8734 - loss: 0.4316 - val_accuracy: 0.9768 - val_loss: 0.0801\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9778 - loss: 0.0745 - val_accuracy: 0.9831 - val_loss: 0.0510\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9868 - loss: 0.0463 - val_accuracy: 0.9832 - val_loss: 0.0527\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9889 - loss: 0.0363 - val_accuracy: 0.9872 - val_loss: 0.0424\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9919 - loss: 0.0274 - val_accuracy: 0.9876 - val_loss: 0.0405\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0498\n",
      "Accuracy: 0.9876000285148621\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Initialize the CNN model\n",
    "model = Sequential([\n",
    "    Input((28, 28, 1)),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8ee84-6a2f-44ee-a544-96d29ae256f3",
   "metadata": {},
   "source": [
    "## 3. **Recurrent Neural Networks (RNN)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac34d401-81b5-4122-a91a-48f782434a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 768ms/step - accuracy: 0.6936 - loss: 0.5755 - val_accuracy: 0.8543 - val_loss: 0.3439\n",
      "Epoch 2/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 952ms/step - accuracy: 0.8993 - loss: 0.2586 - val_accuracy: 0.8504 - val_loss: 0.3464\n",
      "Epoch 3/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 966ms/step - accuracy: 0.9223 - loss: 0.2113 - val_accuracy: 0.8564 - val_loss: 0.3401\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 135ms/step - accuracy: 0.8562 - loss: 0.3456\n",
      "Accuracy: 0.856440007686615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = pad_sequences(X_train, maxlen=500)\n",
    "X_test = pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "# Initialize the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128),\n",
    "    LSTM(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2883957-b2ef-4df0-9fa8-003dcb1a44ae",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Neural Networks, including MLP, CNN, and RNN, offer powerful tools for classification tasks across various domains. MLPs are suitable for general classification problems, CNNs excel in image and spatial data tasks, and RNNs are ideal for sequential and time-series data. Implementing these models in Python using libraries such as Scikit-learn, TensorFlow, and Keras allows for efficient and scalable classification solutions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
